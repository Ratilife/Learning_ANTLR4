# Документация к модулю STFileLexer

## Общее описание

Модуль `STFileLexer` представляет собой лексический анализатор (лексер) для файлов формата ST, автоматически сгенерированный ANTLR 4.13.1 на основе грамматики `STFile.g4`. 
Лексер отвечает за разбиение входного текста на токены (лексемы) - базовые элементы языка, которые затем используются синтаксическим анализатором.

## Основные компоненты

### 1. Класс STFileLexer

Основной класс лексера, наследуемый от `antlr4.Lexer`. Содержит:

#### Атрибуты:
- `atn` - автоматная сеть переходов (ATN)
- `decisionsToDFA` - детерминированные конечные автоматы для принятия решений
- `channelNames` - имена каналов токенов
- `modeNames` - режимы работы лексера
- `literalNames` - буквальные имена токенов
- `symbolicNames` - символические имена токенов
- `ruleNames` - имена правил лексера
- `grammarFileName` - имя файла грамматики

#### Константы токенов:
- `T__0` = 1 - Запятая ','
- `T__1` = 2 - Цифра '0' (маркер шаблона)
- `T__2` = 3 - Цифра '1'
- `BOM` = 4 - Byte Order Mark (сигнатура UTF-8)
- `INT` = 5 - Целое число
- `STRING` = 6 - Текстовая строка
- `LBRACE` = 7 - Открывающая фигурная скобка '{'
- `RBRACE` = 8 - Закрывающая фигурная скобка '}'
- `WS` = 9 - Пробельные символы (whitespace)

### 2. Функция serializedATN()

Возвращает сериализованное представление автоматной сети переходов (ATN) в виде массива чисел. Эта сеть используется для принятия решений при лексическом анализе.

## Детальное описание токенов

### 1. Базовые токены
- `T__0` (',') - разделитель элементов
- `T__1` ('0') - маркер шаблона
- `T__2` ('1') - альтернативный маркер

### 2. Специальные символы
- `BOM` ('\uFEFF') - Byte Order Mark для UTF-8
- `LBRACE` ('{') - начало блока
- `RBRACE` ('}') - конец блока

### 3. Лексические конструкции
- `INT` - последовательность цифр (0-9) или юникод-цифр (65296-65305)
  ```antlr
  [0-9\uFF10-\uFF19]+
  ```
- `STRING` - строки в двойных кавычках с поддержкой экранирования кавычек через ""
  ```antlr
  '"' ('""'|~'"')* '"'
  ```
- `WS` - пробельные символы (пробелы, табы, переносы строк)
  ```antlr
  [ \t\r\n]+
  ```

## Пример работы лексера

Для входного текста:
```st
{ 123, "Sample text" }
```

Лексер сгенерирует последовательность токенов:
1. LBRACE ('{')
2. WS (' ')
3. INT ('123')
4. T__0 (',')
5. WS (' ')
6. STRING ('"Sample text"')
7. WS (' ')
8. RBRACE ('}')

## Особенности реализации

1. **Обработка BOM**: Лексер специально обрабатывает Unicode BOM (Byte Order Mark) как отдельный токен, что важно для корректной обработки UTF-8 файлов.

2. **Поддержка Unicode цифр**: Лексер распознает не только ASCII цифры (0-9), но и их Unicode аналоги (например, ０-９ в полной ширине).

3. **Экранирование кавычек**: В строках двойные кавычки экранируются путем их удвоения (""), что позволяет включать кавычки в содержимое строк.

4. **Игнорирование пробелов**: Пробельные символы отправляются в скрытый канал и не мешают основному разбору.

## Использование

Пример использования лексера:
```python
from antlr4 import InputStream, CommonTokenStream
from STFileLexer import STFileLexer

input_stream = InputStream('{ 123, "text" }')
lexer = STFileLexer(input_stream)
stream = CommonTokenStream(lexer)
stream.fill()  # Заполняем поток токенами

# Вывод всех токенов
for token in stream.tokens:
    print(f"{token.type} ({lexer.symbolicNames[token.type]}): '{token.text}'")
```

## Ограничения

1. Лексер не проверяет семантическую корректность чисел (например, переполнение)
2. Поддерживаются только строки в двойных кавычках
3. Комментарии в исходном файле не поддерживаются

## Генерация лексера

Лексер был сгенерирован автоматически из грамматики `STFile.g4` с помощью команды:
```
antlr4 -Dlanguage=Python3 STFile.g4
```

## Совместимость

Лексер требует:
- Python 3.6+
- Библиотеку antlr4-python3-runtime версии 4.13.1